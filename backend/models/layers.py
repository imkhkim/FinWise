import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter


class HGNN_conv(nn.Module):
    """
    HGNN_conv (HyperGraph Neural Network Convolution Layer)
    ì—­í• : ë¼í”Œë¼ì‹œì•ˆ ì •ê·œí™” í–‰ë ¬ ğº x ì…ë ¥ íŠ¹ì„± ğ‘‹ë¥¼ ì´ìš©í•´ íŠ¹ì„±ì„ ì—…ë°ì´íŠ¸.
    
    êµ¬ì„±: PMI weight -> ì…ë ¥ íŠ¹ì„±ê³¼ ì¶œë ¥ íŠ¹ì„±ì„ ë§¤í•‘í•˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ / bias: ì„ íƒì ìœ¼ë¡œ ì¶”ê°€ë˜ëŠ” í¸í–¥
    ìˆœì „íŒŒ ê³¼ì •: 1) ì…ë ¥ ğ‘‹ì— PMI ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•¨. 2) ğºì™€ ê³±í•˜ì—¬ íŠ¹ì„±ì„ ì „íŒŒ 3) í¸í–¥ì´ ìˆìœ¼ë©´ ë”í•¨.
    """
    def __init__(self, in_ft, out_ft, bias=True):
        super(HGNN_conv, self).__init__()
        self.weight = Parameter(torch.Tensor(in_ft, out_ft))
        if bias:
            self.bias = Parameter(torch.Tensor(out_ft))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)
        if self.bias is not None:
            self.bias.data.uniform_(-stdv, stdv)

    def forward(self, x: torch.Tensor, G: torch.Tensor):
        x = x.matmul(self.weight)
        if self.bias is not None:
            x = x + self.bias
        x = G.matmul(x)
        return x

class HGNN_fc(nn.Module):
    """
    HGNN_fc (Fully Connected Layer for HGNN)
    ì—­í• : HGNNì—ì„œ ë§ˆì§€ë§‰ì— ì‚¬ìš©ë˜ëŠ” Fully Connected Layer -> ë…¸ë“œ ë˜ëŠ” í•˜ì´í¼ì—£ì§€ì˜ íŠ¹ì„±ì„ íŠ¹ì • í´ë˜ìŠ¤ë‚˜ ê°’ìœ¼ë¡œ ë³€í™˜.
    êµ¬ì„±: fc -> ì…ë ¥ íŠ¹ì„±ì„ ì¶œë ¥ í¬ê¸°ë¡œ ë³€í™˜í•˜ëŠ” ì„ í˜• ê³„ì¸µ.
    """
    def __init__(self, in_ch, out_ch):
        super(HGNN_fc, self).__init__()
        self.fc = nn.Linear(in_ch, out_ch)

    def forward(self, x):
        return self.fc(x)

class HGNN_embedding(nn.Module):
    """
    HGNN_embedding:
    ì—­í• : HGNNì—ì„œ ë‘ ê°œì˜ HGNN_conv ë ˆì´ì–´ë¥¼ ìŒ“ì•„, ì…ë ¥ íŠ¹ì„±ì„ ì ì§„ì ìœ¼ë¡œ ë³€í™˜í•˜ê³  í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”©ì„ ìƒì„±.
    êµ¬ì„±: hgc1, hgc2 -> ë‘ ê°œì˜ HGNN_conv ë ˆì´ì–´.
    dropout: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ ì‚¬ìš©.
    ìˆœì „íŒŒ ê³¼ì •: 1) HGNN_convë¥¼ ì ìš©í•˜ê³ , í™œì„±í™” í•¨ìˆ˜(ReLU)ë¥¼ ì‚¬ìš©.
    ë“œë¡­ì•„ì›ƒì„ ì ìš©í•´ ê³¼ì í•© ë°©ì§€.
    ë‘ ë²ˆì§¸ HGNN_convë¥¼ ì ìš©.
    """
    def __init__(self, in_ch, n_hid, dropout=0.5):
        super(HGNN_embedding, self).__init__()
        self.dropout = dropout
        self.hgc1 = HGNN_conv(in_ch, n_hid)
        self.hgc2 = HGNN_conv(n_hid, n_hid)

    def forward(self, x, G):
        x = F.relu(self.hgc1(x, G))
        x = F.dropout(x, self.dropout)
        x = F.relu(self.hgc2(x, G))
        return x

class GraphConvolutionlayer(nn.Module):
    """
    ê·¸ë˜í”„ í•©ì„±ê³± ë ˆì´ì–´ í´ë˜ìŠ¤
    """
    def __init__(self, in_features, out_features, bias=True):
        """
        :param in_features: ì…ë ¥ íŠ¹ì„± ì°¨ì›
        :param out_features: ì¶œë ¥ íŠ¹ì„± ì°¨ì›
        :param bias: í¸í–¥ ì‚¬ìš© ì—¬ë¶€
        """
        super(GraphConvolutionlayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
    
    def reset_parameters(self):
        """
        ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°í™” í•¨ìˆ˜
        """
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self, input, adj):
        """
        ìˆœì „íŒŒ í•¨ìˆ˜
        :param input: ì…ë ¥ íŠ¹ì„±
        :param adj: ì¸ì ‘ í–‰ë ¬
        :return: ê·¸ë˜í”„ í•©ì„±ê³± ê²°ê³¼
        """
        if input.size(1) != self.weight.size(0):  # ì…ë ¥ ë°ì´í„°ì™€ ê°€ì¤‘ì¹˜ ì°¨ì›ì´ ë‹¤ë¥¼ ê²½ìš°
            print(f"Reshaping weight matrix from {self.weight.size()} to match input size {input.size(1)}.")
            self.weight = nn.Parameter(torch.FloatTensor(input.size(1), self.out_features))
            self.reset_parameters()
        
        support = torch.mm(input, self.weight)
        output = torch.spmm(adj, support)
        if self.bias is not None:
            return output + self.bias
        else:
            return output

class GraphAttentionLayer(nn.Module):
    def __init__(self, in_features, out_features):
        super(GraphAttentionLayer, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))
        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))
        self.leakyrelu = nn.LeakyReLU(0.2)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.W.data)
        nn.init.xavier_uniform_(self.a.data)

    def forward(self, h, adj):
        if h.size(1) != self.W.size(0):
            print(f"Reshaping W: {self.W.size()} to match h size: {h.size(1)}")
            self.W = nn.Parameter(torch.zeros(size=(h.size(1), self.out_features)))
            self.reset_parameters()

        Wh = torch.mm(h, self.W)  # (N, in_features) * (in_features, out_features)
        e = self.leakyrelu(torch.mm(Wh, Wh.T))  # (N, N)

        attention = F.softmax(e, dim=1)  # Normalize attention scores
        return torch.mm(attention, Wh)  # Weighted sum of features
